{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(image_size):\n",
    "\n",
    "    transforms_train = albumentations.Compose([\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_val = albumentations.Compose([\n",
    "        albumentations.Resize(image_size, image_size),\n",
    "        albumentations.Normalize()\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(kernel_type, data_dir, train_step):\n",
    "\n",
    "    df = pd.read_csv(data_dir+'/train.csv')\n",
    "\n",
    "#     if train_step == 0:\n",
    "#         df_train = pd.read_csv(os.path.join(data_dir, 'train.csv')).drop(columns=['url'])\n",
    "#     else:\n",
    "#         cls_81313 = df.landmark_id.unique()\n",
    "#         df_train = pd.read_csv(os.path.join(data_dir, 'train.csv')).drop(columns=['url']).set_index('landmark_id').loc[cls_81313].reset_index()\n",
    "        \n",
    "    df['filepath'] = df['image'].apply(lambda x: os.path.join(data_dir, 'train_images', x))\n",
    "#     df = df_train.merge(df, on=['id','landmark_id'], how='left')\n",
    "\n",
    "    label_group2idx = {landmark_id: idx for idx, label_group in enumerate(sorted(df['label_group'].unique()))}\n",
    "    idx2label_group = {idx: label_group for idx, landmark_id in enumerate(sorted(df['label_group'].unique()))}\n",
    "    df['label'] = df['label'].map(label_group2idx)\n",
    "\n",
    "    out_dim = df.label.nunique()\n",
    "    print('out_dim = ',out_dim)\n",
    "    return df, out_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ppb\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.XLMRobertaModel, ppb.XLMRobertaTokenizer, 'xlm-roberta-base')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopDataset(Dataset):\n",
    "    def __init__(self, csv, split, mode, transform=None):\n",
    "\n",
    "        self.csv = csv.reset_index()\n",
    "        self.text_model_pretrained_weights = 'xlm-roberta-base'\n",
    "        self.text_model_tokenizer_class = ppb.XLMRobertaTokenizer\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        title = row.title\n",
    "        label = row.label\n",
    "        image = cv2.imread(filepath)[:,:,::-1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            res = self.transform(image=image)\n",
    "            image = res['image'].astype(np.float32)\n",
    "        else:\n",
    "            image = image.astype(np.float32)\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        \n",
    "        tokenized = tokenizer.encode(title, add_special_tokens=True)\n",
    "        text_max_len = 61\n",
    "#         for i in tokenized.values:\n",
    "#             if len(i) > max_len:\n",
    "#                 max_len = len(i)\n",
    "        if len(tokenized) > text_max_len:\n",
    "            text_max_len = len(tokenized)\n",
    "        \n",
    "        padded = np.array([tokenized + [0]*(max_len-len(tokenized))])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded)  \n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        return torch.tensor(image), torch.tensor(label), input_ids, attention_mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shop",
   "language": "python",
   "name": "shop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
